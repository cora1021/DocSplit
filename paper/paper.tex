% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[review]{EMNLP2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

\usepackage{microtype}
\usepackage{enumitem}
\usepackage{times}
\usepackage{latexsym}
\usepackage{xspace}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{color}
\usepackage{microtype}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{amssymb,amsthm}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{caption}
%\usepackage{paralist}
\usepackage{cleveref}
\usepackage{booktabs}
\usepackage{url}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\newcommand{\todo}[1]{{\color{red}{#1}}}
\newcommand{\our}{\mbox{\textsc{SimDE}}}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{SimDE: Simple Contrastive Learning for Document Embeddings}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

\author{Yujie Wang \\
  Claremont Graduate University 
  \texttt{yujie.wang@cgu.edu} \\\And
  Mike Izbicki \\
  Claremont McKenna College
  \texttt{Michael.Izbicki@ClaremontMcKenna.edu} \\}

\begin{document}
\maketitle
\begin{abstract}
%High-quality text embeddings are essential to various natural language understanding tasks especially for low-resource settings. 
%Existing methods are proposed for general text understanding but representations for long text are less explored.
%Basically, to have better embeddings for long text, a model should have the ability to recognize key information in a long text. 
%We introduce the $\our$ 
%To this end, we present \our, a simple unsupervised contrastive learning framework for long text embeddings in this paper. 
%Specifically, we pretrain a language model to distinguish if two texts have the same topic without any supervision. 
%The positive pairs are constructed by our key information redundancy assumption for long text.
% Experimental results on five datasets show that \our~outperforms the state-of-the-art models by $3.9\%$ on classification tasks and $12.0\%$ macro-F1 in average respectively under general and few-shot text classification settings.

% This paper presents $\our$, the first unsupervised pretraining
% method designed specifically for documents.
% $\our$ uses the contrastive learning framework, and our main contribution is a simple but effective data augmentation technique for generating similar text pairs.
%A key advantage of $\our$ over other techniques for working with
%long text is that it requires no changes to model architecture, and so it is widely applicable.
% Specifically, we pretrain a language model to distinguish if two texts have the same topic without any supervision or specific model architectures, and so it is widely applicable.
%We use $\our$ pretraining to improve the embeddings of previously
%developed state-of-the-art models.
% The positive pairs are constructed by our key information redundancy assumption for documents.
% On standard classification datasets, $\our$ improves all baseline
% models, with an average improvement of $3.9\%$ macro F1 score.
% We also consider a few-shot setting where we show an average
% improvement of $12.0\%$.

Existing pretrained transformer models generate high-quality sentence and paragraph embeddings, but do not work well for large documents.
A growing body of research tackles this problem by proposing new model architectures that are more efficient with large context windows, but all of this research still uses the classic token masking strategy for training.
Token masking is a fundamentally \emph{local} operation in that context of the entire document is rarely needed to correctly predict the masked token---the words much closer to the token will have a much larger
influence.
We propose a novel \emph{global} method for training that requires a model to equally consider all portions of a document.
Our Simple Document Embedding (SimDE) method is based on contrastive learning.
Like token masking, SimDE is fully unsupervised and can work with any existing model architecture.
We experimentally evaluate the SimDE on a wide range of architectures on standard classification datasets for large documents.
We find an average improvement of $3.9\%$ macro F1 score in a standard
regime with large training datasets and a massive $12.0\%$ improvement in the macro F1 score in a few-shot setting.
\end{abstract}



\section{Introduction}

Generating high-quality text embeddings for documents is a long-standing open problem.
Most previous studies focus on either learning sentence-level representations~\cite{Hill2016LearningDR, Logeswaran2018AnEF, Gao2021SimCSESC} where training data usually contain short text or designing specific model structures for larger-range dependencies~\cite{Beltagy2020LongformerTL, Zaheer2020BigBT},
but effective and efficient document representation learning methods are less explored.

\begin{figure}[h]
\includegraphics[width=\columnwidth]{fig/simde.pdf}
\caption{Overall framework of \our. The document is randomly divided into two exclusive subsets of sentences and the two subsets work as positive pairs for contrastive learning. Other instances in the same batch are used as negatives.
}
\label{overall}
\end{figure}

In this paper, we present the $\our$ which is the first unsupervised training method designed specifically for documents.
The training procedure of $\our$ can work with any model architecture to improve document representations. 
Specifically, $\our$ uses contrastive learning,
and our key contribution is a new method for generating positive samples for contrastive learning.
To this end, we first investigate the information redundancy with two methods (details in Appendix~\ref{app:redundacy}) on five datasets for different lengths of documents.
We find (1) information redundancy is larger as the length of the documents is increasing and (2) sentences from the same document usually contain repeated information. 
Based on this observation, we can assume that the model can still learn the semantics of documents even if we drop some sentences. Hence, as shown in Figure~\ref{overall}, we randomly divide the original documents into two parts by sentences as positive pairs. Due to redundancies, our model can still recognize the two pairs have the same semantics. 
The intuition behind this method is that we expect the model will pull representations of two subsets together in the latent space by paying more attention to common keywords so that the model can learn key information from documents automatically. 

To evaluate the quality of document embeddings, we conduct standard and few-shot text classification on five long text datasets involved in News and scientific articles. 
The experimental results show that \our~with two kinds of model structures (i.e.,~BERT and Longformer) can both achieve significant improvements compared to state-of-the-art baselines.

% The recurrent connections in models like the LSTM~\cite{Hochreiter1997LongSM} and GRU~\cite{Cho2014OnTP} allow information at all locations in the document to influence the final embedding,
% but they are slow to train because the models must consider each word sequentially.
% The Transformer architecture~\cite{Vaswani2017AttentionIA} uses self-attention with no recurrent connections,
% and so can be trained much faster than recurrent models.
% This has led to the widespread use of language models like BERT~\cite{Devlin2019BERTPO} and ELMO~\cite{Peters2018DeepCW} that are pre-trained on large scale web data.
% Unfortunately, the memory requirements of self-attention scale as $O(n^2)$ and so are not suitable for long documents.
%Large scale pre-trained transformers like BERT~\cite{} and ELMO~\cite{} are currently state-of-the-art for many NLP tasks that require only short-term dependencies in text.
%These transformers do not handle long-term dependencies well, however, because they require $O(n^2)$ memory for the size of the context window.
% The LongFormer~\cite{Beltagy2020LongformerTL} and BigBird~\cite{Zaheer2020BigBT} models allow larger-range dependencies by introducing improved attention mechanisms that reduce the memory requirements down to $O(n)$.
% This previous research focuses on designing new model architectures suitable for long text,
% but in this paper we present the $\our$ training procedure that can work with any model architecture.
%All of this previous research focuses on designing new model architectures.
%Our paper introduces a new training method called the $\our$ that works for any model architecture.



% $\our$ is the first unsupervised training method designed specifically for long text.
% Previous methods for training on long text either require supervision~\cite{} or use the masked language modeling (MLM) technique~\cite{}.
% In MLM, we replace a token from the input text with a special ``mask'' token and train the model to predict which token was removed.
% The MLM pretraining method has been shown to work well for short-text documents~\cite{},
% but we argue that it sub-optimal for long text because most of the information for predicting the masked token is available near the token.
% The $\our$ pretraining technique is designed to force the model to learn long range dependencies within a document.
% Specifically, $\our$ uses contranstive learning,
% and our key contribution is a new method for generating the positive samples for contrastive learning (see Figure~\ref{overall} above).
%The most similar work to our own is the \texttt{SimCSE} method which uses contrastive learning to pre-train text models,

%Supervised methods cannot be used for large-scale pretraining of models, however, so we present the 

Our paper is organized as follows.
In Section~\ref{sec:method} we formally define the contrastive learning problem and our novel $\our$ training method.
%We highlight how our $\our$ method improves on related contrastive learning frameworks like \texttt{SimCSE}
In Section~\ref{sec:experiments} we develop a new experimental evaluation procedure for documents.
We conclude in Section~\ref{sec:conclusion} by emphasizing that all of our models and datasets are open source.

\section{Method}

\label{sec:method}
In this section, we first formally define contrastive learning, then we describe our \our~method.
\subsection{Contrastive Learning}
Contrastive Learning aims to learn effective representations by pulling semantically close neighbors together and pushing apart non-neighbors in the latent space~\cite{Hadsell2006DimensionalityRB}.
It assumes a contrastive instance $\{x, x^{+}, x_1^{-},\dots,x_{N-1}^{-}\}$ including one positive and $N-1$ negative instances and their representations $\{\mathbf{h}, \mathbf{h}^{+}, \mathbf{h}_1^{-}, \dots,
\mathbf{h}_{N-1}^{-}\}$, where $x$ and $x^{+}$ are semantically related.
we follow the contrastive learning framework~~\cite{Chen2020ASF, Li2022UCTopicUC} and take cross-entropy as our objective function:
\begin{equation}
\label{cl}
    l = -\log \frac{e^{\mathrm{sim}(\mathbf{h}, \mathbf{h}^{+})/\tau}}{e^{\mathrm{sim}(\mathbf{h}, \mathbf{h}^{+})/\tau}+ \sum_{i=1}^{N-1}e^{\mathrm{sim}(\mathbf{h}, \mathbf{h}_i^{-})/\tau}}
\end{equation}
where $\tau$ is a temperature hyperparameter and $\mathrm{sim}(\mathbf{h}_1, \mathbf{h}_2)$ is the cosine similarity $\frac{\mathbf{h}_1^{\top}\mathbf{h}_2}{\Vert \mathbf{h}_1 \Vert \cdot \Vert \mathbf{h}_2 \Vert}$.
In this work, we encode input texts using a pre-trained language model such as BERT~\cite{Devlin2019BERTPO}. Following BERT, we use the first special token \texttt{[CLS]} as the representation of the input and fine-tune all the parameters using the contrastive learning objective in Equation~\ref{cl}.

\subsection{SimDE}
The critical problem in contrastive learning is how to construct positive pairs $(x, x^{+})$.
In representation learning for visual tasks~\cite{Chen2020ASF}, an effective solution is to take two random transformations of the same image (e.g.,~flipping, rotation).
Similarly, in language representations, previous works~\cite{Gao2021SimCSESC, Karpukhin2020DensePR, Meng2021COCOLMCA, Li2022UCTopicUC} apply augmentation techniques such as dropout, word deletion, reordering, and masking.

In this paper, we propose a new method to construct positive instances for documents. 
The basic idea of positive instance construction for contrastive learning is adding random noises to the original data for augmentation. 
The augmented data should have similar representations to the original data. 
Models trained by contrastive losses on augmented data will have an increased ability to learn important features in the data.
To add random noises in documents, we find documents usually has higher information redundancy than sentences (Table~\ref{redundancy} in Appendix). 
With this observation, we can have an assumption: 
the semantics of a document will not be changed even if we drop half of the document. 
We can construct positive pairs under this assumption easily on any text dataset without supervision.
Specifically, for each document in the dataset, we randomly split sentences in the document into two subsets and the two sentence sets do not have intersections. 
In the two subsets, we keep the order of sentences in the original document to form two new documents. 
According to our assumption, the two new documents should have the same semantics and hence they are used as a positive pair in contrastive learning. 

Consider an example (in Figure~\ref{overall}) to understand our positive instance construction process:
Suppose we have a document $T = (s_1, s_2,\dots,s_n)$ where $s_i$ is the $i$-th sentence in document and $n$ is the number of sentences, each sentence will be sent to anchor set or positive set with the same probability ($50\%$). 
The sentences in the same set (i.e.,~anchor or positive) will be concatenated in the same order of $T$ to form one positive pair $(T^+_1, T^+_2)$ for contrastive learning. 
Positive pairs constructed by this method will not contain the same sentence and hence prevent models from overfitting on recognizing the same sentences. 
Instead, models are guided to learn keywords appearing in positive instances so as to improve the ability to recognize key information. 
We split the document at sentence level instead of word level (e.g.,~word deletion for augmentation) because the word-level splitting will cause the discrepancy between pretraining and finetuning and then lead to performance decay.

For negative instances, we use in-batch instances following previous contrastive frameworks~\cite{Gao2021SimCSESC, Li2022UCTopicUC}.



\section{Experiments}

\label{sec:experiments}
In this section, we evaluate the effectiveness of our method by conducting text classification tasks. 
To eliminate the influence of different model structures and focus on the quality of text embeddings. 
We freeze the parameters of different text encoders and fine-tune only a multi-layer perceptron (MLP) to classify the embeddings of text encoders.
We also visualize the attention weights between baselines and \our.

\begin{table}[t]
\centering
    \input{fig/dataset}
    \caption{Statistics of datasets. Ave. and Med. stand for the average and median number of words respectively in one data instance.}
    \label{dataset}
\end{table}

\begin{table*}
    \centering
% \scalebox{0.7}{
% \setlength{\tabcolsep}{1.mm}{
    \input{fig/result.tex}
    % }}
    \caption{For all performance measures, larger numbers are better. Our pre-trained model achieves the best results in all cases.}
    \vspace{-3mm}
    \label{results}
\end{table*}

\subsection{Pretraining Details}
For pre-training, we start from the pretrained BERT-BASE model~\cite{Devlin2019BERTPO} and the Longformer~\cite{Beltagy2020LongformerTL} model~\footnote{The Longformer checkpoint is pretrained on long documents by MLM task and is available from Huggingface.} 
We follow previous works~\cite{Gao2021SimCSESC, Li2022UCTopicUC}: the masked language model (MLM) loss and the contrastive learning loss are used concurrently with in-batch negatives. 
We use English Wikipedia~\footnote{https://en.wikipedia.org/} articles as pretraining data and each article is viewed as one training instance. 
The total number of training instances is 6,218,825. 
Our pretraining learning rate is 5e-5, batch size is 36 and 12 for BERT and Longformer structure respectively. Our model is optimized by AdamW~\cite{Kingma2014AdamAM} in 1 epoch. 
The temperature $\tau$ in the contrastive loss is set to $0.05$ and the weight of MLM is set to $0.1$ following previous work~\cite{Gao2021SimCSESC}.

\subsection{Datasets}
We use the following classic documents datasets to evaluate our method:
(1) Fake News Corpus~\footnote{https://github.com/several27/FakeNewsCorpus};
(2) 20NewsGroups~\cite{Lang1995NewsWeederLT};
(3) arXiv articles dataset~\footnote{https://www.kaggle.com/datasets/Cornell-University/arxiv};
(4) New York Times Annotated Corpus (NYT) ~\cite{sandhaus2008new}; and
(5) BBCNews~\footnote{http://mlg.ucd.ie/datasets/bbc.html}.
We do not use semantic textual similarity (STS) tasks~\cite{Agirre2012SemEval2012T6} because the sentences in these tasks are short which is not suitable to evaluate long text embeddings.

\subsection{Baselines}
We compare our pre-trained model to the baselines of two groups.
(1) BERT based models include BERT~\cite{Devlin2019BERTPO}, SimCSE~\cite{Gao2021SimCSESC}, CT-BERT~\cite{Carlsson2021SemanticRW}. For a fair comparison, we also train a SimCSE with our pretraining dataset (SimCSE$_\mathrm{long}$).
(2) Transformers specified for long sequences include Longformer~\cite{Beltagy2020LongformerTL} and BigBird~\cite{Zaheer2020BigBT}.
We train two versions of \our~with BERT and Longformer (i.e.,~\our$_\mathrm{bert}$ and \our$_{\mathrm{long}}$) for comparison.
We do not include RoBERTa~\cite{Liu2019RoBERTaAR} and IS-BERT~\cite{Zhang2020AnUS} as our baselines because SimCSE achieves better results than these methods according to the paper.

\subsection{Text Classification}
In the standard text classification task, we classify text embeddings with the full training set. 
Training details are in Appendix~\ref{app:details}.

\textbf{Results.}
Table~\ref{results} shows the evaluation results on different datasets. 
Overall, we can see that \our~achieves the best performance over the 5 document datasets and consistently improves the document embeddings with BERT and Longformer structures. 
Specifically, methods pretrained with contrastive objectives (i.e.,~CT-BERT, SimCSE) outperform general language representations (i.e., BERT) which indicates contrastive objectives designed for text embeddings can largely improve the ability of language models to produce high-quality text embeddings. SimCSE pretrained with our document data (i.e.,~SimCSE$_{\mathrm{long}}$) has similar results as the original SimCSE which indicates simplely increasing the length of pretraining text cannot improve document embeddings.
Compared to SimCSE and Longformer, our model achieves $3.9\%$ and $9.4\%$ average macro-F1 improvements with BERT and Longformer structures respectively. Hence, our contrastive learning method is effective for document embeddings.


\subsection{Few-shot Text Classification}
To show the performance of different text embeddings under low-resource settings, we evaluate our model with few-shot training instances.
Training details are in Appendix~\ref{app:details}.

\begin{figure}
\centering
\includegraphics[width=0.8\linewidth]{fig/few_shot.pdf}
\caption{Performance of different models with different numbers of instances per class under few-shot setting.}
\label{few_shot}
\end{figure}

\textbf{Results.} 
Table~\ref{results} shows the results of few-shot text classification on these five datasets. 
We can see that \our~(i.e.,~\our$_{\mathrm{bert}}$ and \our$_{\mathrm{long}}$) achieves $12.0\%$ and $24.3\%$ macro-F1 improvements compared to SimCSE and Longformer respectively. These improvements are higher than standard text classification. 
Besides, we also compare the performance of different baselines and \our$_{\mathrm{bert}}$ with different numbers of training instances on 20News. 
The results in Figure~\ref{few_shot} show the improvements from our method become larger as the number of training instances decreases indicating the importance of high-quality document embeddings for low-resource settings. 
Furthermore, our method achieves the best results under different numbers of training instances.

\subsection{Document Retrieval}
We use document retrieval to evaluate the ability of learning the similarity score between two vectors \cite{ Guo2016ADR}. We follow the document retrieval experiment \cite{Tay2020LongRA} and use the ACL Anthology Network (AAN) \cite{Radev2009TheAA} dataset, which identifies if two papers have a citation link, a common setup used in long-form document matching \cite{Jiang2019SemanticTM,Yang2020Beyond5T}.

\textbf{Results.} 
\begin{table*}
    \centering
    \input{fig/document_rerival.tex}
    \caption{}
    \vspace{-3mm}
    \label{retrieval}
\end{table*}
Table~\ref{retrieval} shows the results of document retrieval experiment on AAN dataset.

\section{Conclusion}
\label{sec:conclusion}
In this work, we propose an unsupervised contrastive learning framework for long text embeddings. 
Our method provides a new method for long text data augmentation without any supervision and language models can get large-scale pretraining on any long text. 
We conduct extensive experiments on text classification tasks under fully supervised and few-shot settings.
Results show that our pre-trained model greatly outperforms state-of-the-art text embeddings, especially when the training data is limited.

\section*{Limitations}
The limitations of our method are as follows:
\begin{enumerate}[nosep,leftmargin=*]
    \item Our method requires human-annotated data, which is expensive and time-consuming.
    \item Our method focuses on document embeddings, we cannot claim the method works well for sentence embeddings.
    \item Due to computing resource limitations, the text used in our work cannot be extremely long to verify the effectiveness of our method in this case.
\end{enumerate}

\section*{Ethics Statement}
We do not anticipate any major ethical concerns; learning document embedding is a fundamental problem in natural language processing. Ethical considerations seem low-risk for the specific datasets studied here because they are all published.

%\section*{Acknowledgements}

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\clearpage
\appendix
\section{Appendix}

\subsection{Related Work}

\textbf{Text Embeddings}. Learning text embeddings is a fundamental task in natural language processing (NLP) which can be used in information retrieval~\cite{Thakur2021BEIRAH}, text similarity~\cite{Gao2021SimCSESC}, classification~\cite{Minaee2021DeepLB}, etc. Existing works focus on learning general text embeddings for different applications. SimCSE~\cite{Gao2021SimCSESC} augment training data by Dropout~\cite{Srivastava2014DropoutAS} to construct positive data for contrastive learning. Sentence-BERT~\cite{Reimers2019SentenceBERTSE} employs siamese and triplet network structures to compare the labeled sentences in the natural language inference (NLI) dataset. Contriever~\cite{Izacard2021UnsupervisedDI} learns text embeddings for information retrieval via contrastive learning considering multilingual corpus. Instructor~\cite{Su2022OneEA} collects a massive dataset from 330 diverse NLP tasks and formulates all tasks into information retrieval tasks with prompts. 
Previous works either rely on massive manually annotated data~\cite{Su2022OneEA, Reimers2019SentenceBERTSE} or focus only on short text (i.e.,~sentences) embeddings~\cite{Gao2021SimCSESC}.
In this paper, we study unsupervised representation learning of document embeddings which usually have longer text than general sentence embeddings and this task brings new challenges for models to understand a document.

\textbf{Model Structures for Documents}. Due to the limitations of Transformer~\cite{Vaswani2017AttentionIA} on long documents, Another line of work studies the efficient model structures which can be effective to encode documents. For example, Longformer~\cite{Beltagy2020LongformerTL} introduces an attention mechanism that scales linearly with sequence length, making it easy to process long documents. BigBird~\cite{Zaheer2020BigBT} proposes a sparse attention mechanism that reduces this quadratic dependency to linear. \citet{Bulatov2022RecurrentMT} combines the memory mechanism in LSTM~\cite{Hochreiter1997LongSM} with Transformer to let transformers encode extremely long sequences. Comparing these methods which focus on designing models for long documents, our work studies the effective training method for long documents which can be adopted to any model structures for pre-training.


\subsection{Redundancy}
\label{app:redundacy}
\begin{table}[t]
    \input{fig/redundancy}
    \caption{Information redundancies for different lengths (i.e.,~word numbers) of text: (1) 0-50 (2) 51-100 (3) 101-200 (4) 201-300 (5) more than 300.}
    \label{redundancy}
\end{table}

We evaluate the redundancy of the text by two methods: (1) counting the repeated verbs and nouns in the text; (2) comparing inner-document and inter-document sentence similarities.

For (1), we first use SpaCy~\footnote{https://spacy.io/} to find verbs and nouns and get their lemmatizations. Intuitively, if the redundancy of a document is high, nouns and verbs will be repeated frequently to express the same topic. Hence, redundancies $R$ in our paper are computed as:
\begin{equation}
    R = \frac{N_{\mathrm{nouns, verbs}}}{D_{\mathrm{nouns, verbs}}}
\end{equation}
where $N_{\mathrm{nouns, verbs}}$ denotes the number of nouns and verbs in a document and $D_{\mathrm{nouns, verbs}}$ is the number of distinct nouns and verbs.

For (2), we compute sentence similarities to compare inner-document and inter-document information. Specifically, we first split documents into sentences and encode these sentences with pre-trained SimCSE~\cite{Gao2021SimCSESC}. Then, we calculate cosine similarities for every two sentences in the same documents (i.e.,~inner-document) and from different documents (i.e.,~inter-document). Results from five datasets are shown in Table~\ref{sent_sim}. We can see that sentences in the same documents have higher similarities compared to sentences from different documents. These results show that sentences in the same documents usually contain repeated information and hence have higher redundancy (i.e.,~sentences expressing similar semantics).

\label{app:sent_sim}
\begin{table}[t]
    \input{fig/sent_sim}
    \caption{Inner-document and inter-document sentences similarities measured by SimCSE.}
    \label{sent_sim}
\end{table}

\subsection{Training Details}
\label{app:details}
For text classification, the learning rate for fine-tuning is 3e-4; the batch size is 8; the maximum sequence length is 512 tokens. 
We fine-tune the last MLP layer on these five datasets and evaluate the classification performance with accuracy and macro-F1 scores.
For few-shot text classification, we sample 10 data instances per class for the FakeNewsCorpus dataset and the arXiv dataset and 5 data instances per class for the other three datasets. 
Other settings are the same as the standard text classification. 
Since there is randomness in sampling, we repeat every experiment 10 times and take the average value of metrics.

\subsection{Attention Weights}
\label{app:attention}
To explore the difference between \our~and other models, we analyze the attention weights of Transformers in different models on the NYT dataset (details in Appendix~\ref{app:attention}). The average weights of different kinds of words are shown in Figure~\ref{attentions}.  We can see that our model has more than $40\%$ higher attention weights on nouns compared to BERT and SimCSE.~\citet{martin-johnson-2015-efficient} shows nouns are more informative than other words in the document understanding. Hence, our pretraining method increases the attention weights of models on nouns which results in higher performance on long text classification. 

\begin{figure}
\centering
\includegraphics[width=0.8\linewidth]{fig/attentions.pdf}
\caption{Attention weights from different models on the NYT dataset.}  
\label{attentions}
\end{figure}

We compute the attention weights for Transformers as follows:
(1) we first extract the attention weights between \texttt{[CLS]} token and all the other tokens;
(2) we compute the averaged weights along different heads in multi-head attention;
(3) the attention weights of the last layer in Transformers are used as the weights for words.
Averaged values are computed for nouns, verbs, adjectives, and other words.

\end{document}
