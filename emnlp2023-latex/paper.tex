% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[review]{EMNLP2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{float}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
% \usepackage{inconsolata}

\usepackage{microtype}
\usepackage{enumitem}
\usepackage{times}
\usepackage{latexsym}
\usepackage{xspace}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{color}
\usepackage{microtype}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{amssymb,amsthm}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{caption}
%\usepackage{paralist}
\usepackage{cleveref}
\usepackage{booktabs}
\usepackage{url}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\newcommand{\todo}[1]{{\color{red}{#1}}}
\newcommand{\our}{\mbox{\textsc{DocSplit}}}
\newcommand{\ourbert}{$\our_{\mathrm{bert}}$}
\newcommand{\ourlong}{$\our_{\mathrm{long}}$}

\newcommand{\fixme}[1]{{\color{red}{FIXME: {#1}}}}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{DocSplit: Simple Contrastive Pretraining for \\ Large Document Embeddings}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

\author{Yujie Wang \\
  Claremont Graduate University 
  \texttt{yujie.wang@cgu.edu} \\\And
  Mike Izbicki \\
  Claremont McKenna College
  \texttt{Michael.Izbicki@ClaremontMcKenna.edu} \\}

\begin{document}
\maketitle
\begin{abstract}

Existing model pretraining methods only consider local information.
For example, in the popular token masking strategy, the words closer to the masked token are more important for prediction than words far away.
This results in pretrained models that generate high-quality sentence embeddings, but low-quality embeddings for large documents.
We propose a new pretraining method called $\our$ which forces models to consider the entire global context of a large document.
Our method uses a contrastive loss where the positive examples are randomly sampled sections of the input document, and negative examples are randomly sampled sections of unrelated documents.
Like previous pretraining methods, $\our$ is fully unsupervised, easy to implement, and can be used to pretrain any model
architecture.
Our experiments show that $\our$ outperforms other pretraining methods for document classification, few shot learning, and information retrieval tasks.
\end{abstract}



\section{Introduction}

Generating high-quality text embeddings for documents is a long-standing open problem.
Most previous studies focus on either learning sentence-level representations~\cite{Hill2016LearningDR, Logeswaran2018AnEF, Gao2021SimCSESC} where training data usually contain short text or designing specific model structures for larger-range dependencies~\cite{Beltagy2020LongformerTL, Zaheer2020BigBT},
but effective and efficient document representation learning methods are less explored.

\begin{figure}
\includegraphics[width=\columnwidth]{fig/docsplit.pdf}
    %\includegraphics[width=\columnwidth]{example-image-golden.pdf}
\caption{
    Document splitting ($\our$) is a new pretraining strategy that uses contrastive learning.
    The hard part of contrastive learning is generating positive instances.
    $\our$ generates these pairs from an input data point by randomly assigning each sentence in the input document to one of the pairs.
    %For each sentence in the input document, the sentence is randomly assigned to one of the positive instances ({\color{blue}{$T_1^+$} or \color{green}{$T_2^+$}).
    %Overall framework of \our. The document is randomly divided into two exclusive subsets of sentences and the two subsets work as positive pairs for contrastive learning. Other instances in the same batch are used as negatives.
}
\label{overall}
\end{figure}

In this paper, we present the $\our$ which is the first unsupervised pretraining method designed specifically for large documents.
The training procedure of $\our$ can work with any model architecture to improve document representations. 
Specifically, $\our$ uses contrastive learning,
and our key contribution is a new method for generating positive samples for contrastive learning.
%To this end, we first investigate the information redundancy with two methods (details in Appendix~\ref{app:redundacy}) on five datasets for different lengths of documents.
%We find (1) information redundancy is larger as the length of the documents is increasing and (2) sentences from the same document usually contain repeated information. 
%Based on this observation, we can assume that the model can still learn the semantics of documents even if we drop some sentences. Hence, as shown in Figure~\ref{overall}, we randomly divide the original documents into two parts by sentences as positive pairs. Due to redundancies, our model can still recognize the two pairs have the same semantics. 
%The intuition behind this method is that we expect the model will pull representations of two subsets together in the latent space by paying more attention to common keywords so that the model can learn key information from documents automatically. 

To evaluate the quality of document embeddings, we conduct standard and few-shot text classification on five large document datasets involved in News and scientific articles. Moreover, to further validate the effectiveness of our method, we also conduct document retrieval on the AAN dataset which is designed for long document understanding evaluation.
The experimental results show that \our~with two kinds of model structures (i.e.,~BERT and Longformer) can both achieve significant improvements compared to state-of-the-art baselines.

% The recurrent connections in models like the LSTM~\cite{Hochreiter1997LongSM} and GRU~\cite{Cho2014OnTP} allow information at all locations in the document to influence the final embedding,
% but they are slow to train because the models must consider each word sequentially.
% The Transformer architecture~\cite{Vaswani2017AttentionIA} uses self-attention with no recurrent connections,
% and so can be trained much faster than recurrent models.
% This has led to the widespread use of language models like BERT~\cite{Devlin2019BERTPO} and ELMO~\cite{Peters2018DeepCW} that are pre-trained on large scale web data.
% Unfortunately, the memory requirements of self-attention scale as $O(n^2)$ and so are not suitable for long documents.
%Large scale pre-trained transformers like BERT~\cite{} and ELMO~\cite{} are currently state-of-the-art for many NLP tasks that require only short-term dependencies in text.
%These transformers do not handle long-term dependencies well, however, because they require $O(n^2)$ memory for the size of the context window.
% The LongFormer~\cite{Beltagy2020LongformerTL} and BigBird~\cite{Zaheer2020BigBT} models allow larger-range dependencies by introducing improved attention mechanisms that reduce the memory requirements down to $O(n)$.
% This previous research focuses on designing new model architectures suitable for long text,
% but in this paper we present the $\our$ training procedure that can work with any model architecture.
%All of this previous research focuses on designing new model architectures.
%Our paper introduces a new training method called the $\our$ that works for any model architecture.



% $\our$ is the first unsupervised training method designed specifically for long text.
% Previous methods for training on long text either require supervision~\cite{} or use the masked language modeling (MLM) technique~\cite{}.
% In MLM, we replace a token from the input text with a special ``mask'' token and train the model to predict which token was removed.
% The MLM pretraining method has been shown to work well for short-text documents~\cite{},
% but we argue that it sub-optimal for long text because most of the information for predicting the masked token is available near the token.
% The $\our$ pretraining technique is designed to force the model to learn long range dependencies within a document.
% Specifically, $\our$ uses contranstive learning,
% and our key contribution is a new method for generating the positive samples for contrastive learning (see Figure~\ref{overall} above).
%The most similar work to our own is the \texttt{SimCSE} method which uses contrastive learning to pre-train text models,

%Supervised methods cannot be used for large-scale pretraining of models, however, so we present the 

Our paper is organized as follows.
In Section~\ref{sec:method} we formally define the contrastive learning problem and our novel $\our$ training method.
%We highlight how our $\our$ method improves on related contrastive learning frameworks like \texttt{SimCSE}
In Section~\ref{sec:experiments} we develop a new experimental evaluation procedure for documents.
We conclude in Section~\ref{sec:conclusion} by emphasizing that all of our models and datasets are open source.

\section{Method}

\label{sec:method}
In this section, we first formally define contrastive learning, then we describe our \our~method.

%\subsection{Overview of Contrastive Learning}
%
%Contrastive learning is a method for unsupervised pretraining of deep neural networks.
%The idea was first introduced by \citet{Hadsell2006DimensionalityRB},
%but saw a resurgence in popularity when it was used to pretrain multi-purpose vision models \cite{Chen2020ASF}.
%It then became widely used in the natural language processing community to generate state-of-the-art pretrained models.
%
%%The original BERT transformer architecture \citep{Devlin2019BERTPO} was pretrained with token masking.
%In this section we provide an intuitive, non-mathematical overview of contrastive learning.
%Contrastive learning is a 

\subsection{Contrastive Learning}
Contrastive Learning aims to learn effective representations by pulling semantically close neighbors together and pushing apart non-neighbors in the latent space~\cite{Hadsell2006DimensionalityRB}.
It assumes a contrastive instance $\{x, x^{+}, x_1^{-},\dots,x_{N-1}^{-}\}$ including one positive and $N-1$ negative instances and their representations $\{\mathbf{h}, \mathbf{h}^{+}, \mathbf{h}_1^{-}, \dots,
\mathbf{h}_{N-1}^{-}\}$, where $x$ and $x^{+}$ are semantically related.
we follow the contrastive learning framework~~\cite{Chen2020ASF, Li2022UCTopicUC} and take cross-entropy as our objective function:
\begin{equation}
\label{cl}
    l = -\log \frac{e^{\mathrm{sim}(\mathbf{h}, \mathbf{h}^{+})/\tau}}{e^{\mathrm{sim}(\mathbf{h}, \mathbf{h}^{+})/\tau}+ \sum_{i=1}^{N-1}e^{\mathrm{sim}(\mathbf{h}, \mathbf{h}_i^{-})/\tau}}
\end{equation}
where $\tau$ is a temperature hyperparameter and $\mathrm{sim}(\mathbf{h}_1, \mathbf{h}_2)$ is the cosine similarity $\frac{\mathbf{h}_1^{\top}\mathbf{h}_2}{\Vert \mathbf{h}_1 \Vert \cdot \Vert \mathbf{h}_2 \Vert}$.

%In this work, we encode input texts using a pre-trained language model such as BERT~\cite{Devlin2019BERTPO}. Following BERT, we use the first special token \texttt{[CLS]} as the representation of the input and fine-tune all the parameters using the contrastive learning objective in Equation~\ref{cl}.

For negative instances, we use in-batch instances following previous contrastive frameworks~\cite{Gao2021SimCSESC, Li2022UCTopicUC}.
The critical problem in contrastive learning is how to construct positive pairs $(x, x^{+})$.
%In representation learning for visual tasks~\cite{Chen2020ASF}, an effective solution is to take two random transformations of the same image (e.g.,~flipping, rotation).
%Similarly, in language representations, previous works~\cite{Gao2021SimCSESC, Karpukhin2020DensePR, Meng2021COCOLMCA, Li2022UCTopicUC} apply augmentation techniques such as dropout, word deletion, reordering, and masking.

\subsection{Document Splitting ($\our$)}
Our main contribution is the $\our$ method to construct positive instances for documents. 
The idea is simple to describe and implement.
Given an input document, we first split the document into sentences.
Then each sentence is randomly assigned to eith

There are several intuitive reasons why we should expect $\our$
~to generate good document pairs.
First, we can think of each positive instance as a summary of the original document.
Second, our splitting strategy captures global information about the document.

%The basic idea of positive instance construction for contrastive learning is adding random noises to the original data for augmentation. 
%The augmented data should have similar representations to the original data. 
%Models trained by contrastive losses on augmented data will have an increased ability to learn important features in the data.
%To add random noises in documents, we find documents usually has higher information redundancy than sentences (Table~\ref{redundancy} in Appendix). 
%With this observation, we can have an assumption: 
%the semantics of a document will not be changed even if we drop half of the document. 
%We can construct positive pairs under this assumption easily on any text dataset without supervision.
%Specifically, for each document in the dataset, we randomly split sentences in the document into two subsets and the two sentence sets do not have intersections. 
%In the two subsets, we keep the order of sentences in the original document to form two new documents. 
%According to our assumption, the two new documents should have the same semantics and hence they are used as a positive pair in contrastive learning. 
%
%Consider an example (in Figure~\ref{overall}) to understand our positive instance construction process:
%Suppose we have a document $T = (s_1, s_2,\dots,s_n)$ where $s_i$ is the $i$-th sentence in document and $n$ is the number of sentences, each sentence will be sent to anchor set or positive set with the same probability ($50\%$). 
%The sentences in the same set (i.e.,~anchor or positive) will be concatenated in the same order of $T$ to form one positive pair $(T^+_1, T^+_2)$ for contrastive learning. 
%Positive pairs constructed by this method will not contain the same sentence and hence prevent models from overfitting on recognizing the same sentences. 
%Instead, models are guided to learn keywords appearing in positive instances so as to improve the ability to recognize key information. 
%We split the document at sentence level instead of word level (e.g.,~word deletion for augmentation) because the word-level splitting will cause the discrepancy between pretraining and finetuning and then lead to performance decay.

\section{Related Work}
\label{sec:related}

There are two categories of related work:
models trained using contrastive learning,
and models designed for large documents.

\subsection{Contrastive Pretraining}
\label{sec:contrastive}

Contrastive learning has shown remarkable recent success for developing sentence embeddings.
The simplest method is SimCSE \cite{Gao2021SimCSESC},
which uses dropout to generate correlated positive samples.
The contrastive tension method \cite{Carlsson2021SemanticRW} is similarly generic,
but has a much more complicated implementation involving multiple models trained jointly.
Because both of these pretraining strategies are generic,
they can be used with any type of input including documents;
but they do not take explicit advantage of document structure.
%CT-BERT \cite{Carlsson2021SemanticRW} uses a pair of training models (contrastive tension).
The INSTRUCTOR model \cite{Su2022OneEA} is the current state-of-the-art model for most downstream tasks.
The contrastive objective for this model requires a specially constructed corpus of manual-human annotations,
and this corpus is limited only to sentence-level annotations instead of document-level annotations.
We show that our model significantly improves on INSTRUCTOR on document level tasks, and it is not clear how to extend the INSTRUCTOR model to document-level tasks because human annotation for documents is significantly more expensive than for sentences.
The Contriever model \cite{Izacard2021UnsupervisedDI} uses a contrastive objective most similar to our own.
They use the inverse cloze and document cropping tasks for pretraining.
In document cropping, a document is divided in half and the two halves are used as the positive samples;
in inverse cloze, a contiguous substring of the document is used as one positive sample and all other strings are used as the negative sample.
The $\our$ pretraining method can be seen as a generalization of these methods.
%It is evaluated only on information retrieval setting.
%INSTRUCTOR~\cite{Su2022OneEA} uses instruction tuning.
%They create a manually annotated corpus of 330 different datasets.

\subsection{Large Document Architectures}

%In order to demonstrate that $\our$ is widely applicable,
%we pretrain two models using two different architectures.

All of the models discussed in Section \ref{sec:contrastive} above are based off of the BERT architecture \cite{Devlin2019BERTPO}.
This architecture uses an attention mechanism that requires $O(n^2)$ memory and runtime,
where $n$ is the size of the attention window.
The maximum size of a document that a model can understand is limited by this window size,
and so compute for these models scales quadratically with the length of the documents.

A growing body of research focuses on developing new architectures with reduced computational requirements that enable processing larger documents.
The LongFormer \cite{Beltagy2020LongformerTL} and BigBird \cite{Zaheer2020BigBT} models pioneered this line of research,
and both models reduce the runtime of the attention mechanism to $O(n)$.
%A variety of other architectures have subsequently been proposed like the Informer \cite{zhou2021informer}, Switch Transformer \cite{fedus2022switch}, Performer \cite{choromanski2020rethinking}, and LinFormer \cite{wang2020linformer}
A variety of other architectures have subsequently been proposed \cite[e.g.][]{zhou2021informer,fedus2022switch,choromanski2020rethinking,wang2020linformer,liu2022ecoformer}.
\citet{tay2022efficient} provide a survey of this large body of work.
Importantly, all of this research focuses only on improving the computational aspects of model architecture,
and none of these models use a training objective designed specifically for large documents.
Because the $\our$~pretraining method is model agnostic,
we can easily apply it to any of these newly proposed model architectures.
For computational reasons, we limit our experimental comparisons in Section \ref{sec:experiments} below to the LongFormer and BigBird models since these are the two most influential model architectures designed for large documents.
We find a large performance improvement when these models are trained with $\our$,
and expect this performance improvement would extend to similar models as well.

%We call the second model \ourlong~because it is pretrained on the LongFormer architecture \cite{Beltagy2020LongformerTL}.
%LongFormer uses a modified attention head that requires only $O(n)$ compute instead of the standard $O(n^2)$ that Bert uses,
%and so can handle larger context windows needed for large documents.
%Our \ourlong pretrained model inherits this computational advantage.
%We also compare to the BigBird~\cite{Zaheer2020BigBT} model,
%which is architecturally similar.


\section{Experiments}

\label{sec:experiments}

%To properly evaluate our $\our$~pretraining method against the related work described in Section \ref{sec:related} above,
%we pretrain two different models.
%The first model \ourbert~uses the same BERT architecture that all other contrastive models of Section \ref{sec:contrastive} were trained on.
%The second model \ourlong~uses the LongFormer architecture.

We perform a careful ablation study to isolate the effects of the $\our$ pretraining method on downstream task performance.
First, we pretrain separate models for each group of baseline models described in Section \ref{sec:related} above.
%In particular, the \ourbert~model is trained on the BERT architecture and allows us to directly compare the performance of all contrastively pretrained models of \ref{sec:contrastive}.
%\footnote{We do not compare to the popular RoBERTa~\cite{Liu2019RoBERTaAR}, IS-BERT~\cite{Zhang2020AnUS} and GTR~\cite{Ni2021LargeDE} models because SimCSE and INSTRUCTOR achieve better results than these methods according to their papers.}
%The \ourlong~model is trained on the LongFormer architecture and allows to see how better pretraining drastically improves the performance of these architectures designed specifically for large documents.
Then, we perform downstream experiments on standard classification, few-shot learning, and information retrieval tasks.
In all cases, our pretrained models significantly outperform prior work.


%We first describe how we pretrain our model.
%Then we describe our classification, few-shot learning, and information retrieval evaluation tasks.

%We use a standard experimental setup to evaluate the effectiveness of \our.
%First, we pretr
%
%We evaluate the effectiveness of \our~using a standard procedureby pretraining two models on English language wikipedia.
%Then we finetune the model
%
%In this section, we evaluate the effectiveness of our method by conducting text classification tasks. 
%To eliminate the influence of different model structures and focus on the quality of text embeddings. 
%We freeze the parameters of different text encoders and fine-tune only a multi-layer perceptron (MLP) to classify the embeddings of text encoders.
%We also visualize the attention weights between baselines and \our.
%
%We do not use semantic textual similarity (STS) tasks~\cite{Agirre2012SemEval2012T6} because the sentences in these tasks are short which is not suitable to evaluate large document embeddings.
\begin{table*}
    \centering
% \scalebox{0.7}{
% \setlength{\tabcolsep}{1.mm}{
    \input{fig/result-simplified.tex}
    % }}
    \caption{
        In the text classification of Experiment 1, models pretrained with $\our$ outperform baseline models in all cases.
        Larger numbers are better.
    }
    \vspace{-3mm}
    \label{results}
\end{table*}

\subsection{Pretraining Details}

We pretrain two models on two different architectures.
All prior work using contrastive learning discussed in Section \ref{sec:contrastive} above evaluates their pretraining methods on the BERT architecture.
To fairly compare against these methods, we pretrain our \ourbert~model also on this architecture.
Ultimately, however, we are interested in large document performance, and so we expect that a model architecture designed specifically for large documents will improve performance.
We therefore also pretrain the \ourlong~model on the LongFormer architecture.
This second model will be used to evaluate against other models designed specifically for large documents.

To pretrain both models, we follow the standard pretraining procedure for contrastive losses established by \citet{Gao2021SimCSESC} and \citet{Li2022UCTopicUC}.
We simultaneously optimize both the masked language model (MLM) loss (with weight$=0.1$) and the contrastive loss (with temperature $\tau=0.05$).
We use English Wikipedia articles as our pretraining dataset.
These articles are long, and so we expect that a pretraining procedure designed for large documents will improve performance.
The total number of training instances is 6,218,825.
We use AdamW \cite{Kingma2014AdamAM} with a learning rate of 5e-5.
\ourbert uses a batch size of 36.
And due to the larger memory requirements of the LongFormer architecture,
\ourlong uses a batchsize of 12.
For both models, we know that performance improvements on downstream tasks must be due to the pretraining procedure and not the dataset because all baseline models include English language wikipedia in their training set.

%\subsection{Baselines}
%
%We compare our two pre-trained model to baselines with similar architectures and different pretraining strategies.
%
%
%For \ourbert
%(1) Language models trained for general language understanding and sentence embeddings include BERT~\cite{Devlin2019BERTPO}, SimCSE~\cite{Gao2021SimCSESC}~\footnote{SimCSE can be viewed as an ablation model without document splitting.}, CT-BERT~\cite{Carlsson2021SemanticRW}, Contriever~\cite{Izacard2021UnsupervisedDI} and INSTRUCTOR~\cite{Su2022OneEA}. 
%For a fair comparison, we also train a SimCSE with our pretraining dataset (SimCSE$_\mathrm{long}$).
%(2) Transformers specified for long sequences include Longformer~\cite{Beltagy2020LongformerTL} and BigBird~\cite{Zaheer2020BigBT}.
%We train two versions of \our~with BERT and Longformer (i.e.,~\our$_\mathrm{bert}$ and \our$_{\mathrm{long}}$) for comparison.


\subsection{Experiment 1: Text Classification}
%In the standard text classification task, we classify text embeddings with the full training set. 
%Training details are in Appendix~\ref{app:details}.
%
%\textbf{Datasets.}
We fine tune \ourbert, \ourlong, and all baseline models on five standard document datasets.
The datasets are summarized in the table below:%
\footnote{Citations for the datasets are:
Fake News Corpus \url{https://github.com/several27/FakeNewsCorpus};
arXiv articles dataset~\url{https://www.kaggle.com/datasets/Cornell-University/arxiv};
20NewsGroups~\cite{Lang1995NewsWeederLT};
New York Times Annotated Corpus (NYT) ~\cite{sandhaus2008new}; and
BBCNews~\url{http://mlg.ucd.ie/datasets/bbc.html}.
}

\begin{table}[H]
\centering
~~\input{fig/dataset}
\label{dataset}
\end{table}

\noindent
Notice in particular that every dataset has documents that exceed the standard context window length of the BERT model.
For these documents, we follow the standard procedure and truncate the documents before passing them to BERT-based models.

%\textbf{Results.}
Table~\ref{results} shows the accuracy and F1 score of every model on these datasets.
Notice that \ourbert~out performs all BERT-based models discussed in Section \ref{sec:contrastive},
and \ourlong~outperforms all models on every dataset.
There are no results for the INSTRUCTOR model on the 20News dataset because INSTRUCTOR was pretrained on this dataset and the authors state that evaluating INSTRUCTOR on datasets it was pretrained is incorrect due to data contamination.
%Overall, we can see that \our~achieves the best performance over the 4 document datasets (except INSTRUCTOR on 20News due to its data leakage) and consistently improves the document embeddings with BERT and Longformer structures. 
%INSTRUCTOR~\cite{Su2022OneEA} includes 20News in their pretraining datasets which causes data leakage in our experiments, hence INSTRUCTOR results on 20News are not comparable to others.
%Specifically, methods pretrained with contrastive objectives (i.e.,~CT-BERT, SimCSE, Contriever and INSTRUCTOR) outperform general language representations (i.e., BERT and LongFormer) which indicates contrastive objectives designed for text embeddings can largely improve the ability of language models to produce high-quality text embeddings. 
%SimCSE pretrained with our document data (i.e.,~SimCSE$_{\mathrm{long}}$) has similar results as the original SimCSE which indicates simply increasing the length of pretraining text cannot improve document embeddings.
%Under the same model structures, our model achieves $3.9\%$ and $9.4\%$ average macro-F1 improvements with BERT and Longformer structures respectively compared to SimCSE and Longformer. 
%Compared to the state-of-the-art model INSTRUCTOR, our model (\our$_{\mathrm{bert}}$) can achieve $3.4\%$ improvements.
%Hence, our contrastive learning method is effective for document embeddings.


\subsection{Experiment 2: Few-shot Learning}

Next we evaluate how $\our$~pretraining performs on classification tasks with a small number of training examples.
We follow the standard procedure of artificially limiting the number of training examples used during training,
and evaluating on the same test set.
Figure 2 shows the classification accuracy on the 20News dataset as we vary the size of the training set.
We see that \ourbert~outperforms all other BERT-based models accross all sample sizes.

The results on other datasets and for LongFormer-based models are similar.
A full set of results on other datasets is available in the Appendix.

\begin{figure}
\centering
\includegraphics[width=0.8\linewidth]{fig/few_shot.pdf}
\caption{
    \ourbert~outperforms all other BERT-based models in a few-shot classification task on the 20News dataset.
    \fixme{change x-axis to ``training examples per class''; change $\our$ to \ourbert, you can export the figure in tex instead of pdf to get the latex support needed for the legend}
}
\label{few_shot}
\end{figure}

%\textbf{Results.} 
%Table~\ref{results} shows the results of few-shot text classification on these five datasets. 
%We can see that, under the same model structure, \our~(i.e.,~\our$_{\mathrm{bert}}$ and \our$_{\mathrm{long}}$) achieves $12.0\%$ and $24.3\%$ macro-F1 improvements compared to SimCSE and Longformer respectively. 
%Surprisingly, INSTRUCTOR achieves low performance on the NYT and BBCNews under few-shot settings.
%%and achieves 0.028+0.082+2.03+0.769. 
%These improvements are higher than standard text classification. 
%Besides, we also compare the performance of different baselines (using BERT as a backbone) and \our$_{\mathrm{bert}}$ with different numbers of training instances on 20News. 
%The results in Figure~\ref{few_shot} show the improvements from our method become larger as the number of training instances decreases indicating the importance of high-quality document embeddings for low-resource settings. 
%Furthermore, our method achieves the best results under different numbers of training instances.

\subsection{Experiment 3: Document Retrieval}
We conduct document retrieval to evaluate the ability to learn the similarity score between two vectors~\cite{Guo2016ADR} of documents. We follow the document retrieval experiment~\cite{Tay2020LongRA} and use the ACL Anthology Network (AAN)~\cite{Radev2009TheAA} dataset, which identifies if two papers have a citation link, a common setup used in long-form document matching~\cite{Jiang2019SemanticTM,Yang2020Beyond5T}.
Specifically, for all baselines, we use models to obtain document embeddings and finetune an MLP layer on two concatenated embeddings to predict if two documents have a citation link.

\textbf{Results.} 
\begin{table}
    \centering
    \input{fig/document_rerival.tex}
    \caption{}
    \vspace{-3mm}
    \label{retrieval}
\end{table}
Table~\ref{retrieval} shows the results of the document retrieval experiment on AAN dataset.
We can find that our method achieves the best results due to the effectiveness of our pretraining methods based on document splitting.

\section{Conclusion}
\label{sec:conclusion}
In this work, we propose an unsupervised contrastive learning framework for large document embeddings. 
Our paper provides a new method for large document data augmentation without any supervision and language models can get large-scale pretraining on any large documents. 
We conduct extensive experiments on text classification tasks under fully supervised and few-shot settings and a document retrieval task for further evaluation.
Results show that our pre-trained model greatly outperforms state-of-the-art text embeddings, especially when the training data is limited.

\section*{Limitations}
The limitations of our method are as follows:
\begin{enumerate}[nosep,leftmargin=*]
    %\item Our method requires human-annotated data, which is expensive and time-consuming.
    \item Generative models cannot be trained using $\our$, but they can be trained using token masking.
    \item We do not know how $\our$ will generalize to extremely large documents (e.g.\ book length).
        We lack the computational resources to train models on these larger documents.
\end{enumerate}

\section*{Ethics Statement}

Learning embeddings is a standard problem in natural language processing.
Our approach uses standard training datasets and training procedures.
There are therefore no direct ethical concerns with this research.
%We hope that our pretrained models and pretraining strategies will become widely used,
%but we have no control over whether
%As with all tools, however, the embeddings our model provides can be used for either good or evil.

Our total compute is relatively small.
Pretraining our \ourbert~and \ourlong~models took about 1 month each on a standard desktop system with an NVidia 2080 GPU.
Other models use larger GPU clusters that require considerably more energy.
The finetuning procedures run in less than 1 day on the same system.

%We do not anticipate any major ethical concerns; learning document embedding is a fundamental problem in natural language processing. Ethical considerations seem low-risk for the specific datasets studied here because they are all published.

%\section*{Acknowledgements}

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\clearpage
\appendix
\section{Appendix}

\subsection{Training Details}
\label{app:details}
For text classification, the learning rate for fine-tuning is 3e-4; the batch size is 8; the maximum sequence length is 512 tokens. 
We fine-tune the last MLP layer on these five datasets and evaluate the classification performance with accuracy and macro-F1 scores.
For few-shot text classification, we sample 10 data instances per class for the FakeNewsCorpus dataset and the arXiv dataset and 5 data instances per class for the other three datasets. 
Other settings are the same as the standard text classification. 
Since there is randomness in sampling, we repeat every experiment 10 times and take the average value of metrics.

% \subsection{Attention Weights}
% \label{app:attention}
% To explore the difference between \our~and other models, we analyze the attention weights of Transformers in different models on the NYT dataset (details in Appendix~\ref{app:attention}). The average weights of different kinds of words are shown in Figure~\ref{attentions}.  We can see that our model has more than $40\%$ higher attention weights on nouns compared to BERT and SimCSE.~\citet{martin-johnson-2015-efficient} shows nouns are more informative than other words in the document understanding. Hence, our pretraining method increases the attention weights of models on nouns which results in higher performance on long text classification. 

% \begin{figure}
% \centering
% \includegraphics[width=0.8\linewidth]{fig/attentions.pdf}
% \caption{Attention weights from different models on the NYT dataset.}  
% \label{attentions}
% \end{figure}

% We compute the attention weights for Transformers as follows:
% (1) we first extract the attention weights between \texttt{[CLS]} token and all the other tokens;
% (2) we compute the averaged weights along different heads in multi-head attention;
% (3) the attention weights of the last layer in Transformers are used as the weights for words.
% Averaged values are computed for nouns, verbs, adjectives, and other words.

\end{document}
